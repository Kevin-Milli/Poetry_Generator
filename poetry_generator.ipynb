{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973a1014-2479-4d33-8d09-1ebf1876456f",
   "metadata": {},
   "source": [
    "# poetry generator\n",
    "\n",
    "Through a collection of poems by *Robert Frost*, the goal of this notebook is to generate poetry using the Markov model\n",
    "\n",
    "The `Markov model` that will be used is of first and second order, so the variable will not depend solely on the state preceding it but also on the state preceding its predecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d663246-3349-4ea5-bd25-5c479ee7d25b",
   "metadata": {},
   "source": [
    "## Markov Model\n",
    "\n",
    "The Markov model is a class of statistical models used to describe sequences of events where the probability of a subsequent event depends only on the current state and is independent of previous states. In other words, a Markov process satisfies the Markov property, which is known as the property of the future being dependent only on the present.\n",
    "\n",
    "A Markov process is defined by:\n",
    "\n",
    "1) `States`: A finite set of possible states (often denoted by letters, numbers, or names).\n",
    "\n",
    "2) `Transition Matrix`: A matrix representing the transition probabilities between states. Each element (i, j) of the matrix represents the probability of transitioning from state i to state j in a single step.\n",
    "\n",
    "`Markov chains` are a common type of Markov process in which states and transitions between them are used to model sequential behavior in a wide range of applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeb19ab7-51cc-44c5-960d-5917890e0a43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import string\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3be92582-6c8e-447b-a814-ee73b1ffd5c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial = {}           # start of a phrase\n",
    "first_order = {}       # first order transiction probabilities for second word \n",
    "second_order = {}      # second oreder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3ec6d1-5276-43d9-bb29-74b2fcdd059b",
   "metadata": {},
   "source": [
    "**`RemovePunctuation`:** Function that makes the text more manageable for processing, removing punctuation, converting the text to lowercase, and eliminating possible spaces. Finally, using the `split()` function to separate each individual word into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d712214-87ec-4a02-8a1e-464edd8574e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text_and_tokenize(text):\n",
    "    text = text.rstrip().lower()\n",
    "    return text.translate(str.maketrans('', '', string.punctuation)).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a144e84-fff7-418c-9080-7fec917355d8",
   "metadata": {},
   "source": [
    "**Add2Dict:** Function that checks if the element is present in the passed dictionary; if it is not, it creates the element and finally adds the value passed to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9683b581-5ca3-40c8-bc42-37d6c1c0bdbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Add2Dict(dict_, key, value):\n",
    "    if key not in dict_:\n",
    "        dict_[key] = []\n",
    "    dict_[key].append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b113516-5fad-4536-8551-9a9714552490",
   "metadata": {},
   "source": [
    "**List2ProbDict:** Function that calculates the percentage of each element based on its distribution in the passed list. This way, I can assign each element its respective probability in a subsequent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a910d7d4-2da9-43b8-ac46-1f21aa68d929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def List2ProbDict(word_list):\n",
    "    # Convert each list of possibilities into a dictionary of probabilities\n",
    "    word_prob_dict = {}\n",
    "    total_words = len(word_list)\n",
    "    \n",
    "    for word in word_list:\n",
    "        word_prob_dict[word] = word_prob_dict.get(word, 0.0) + 1\n",
    "    \n",
    "    for word, count in word_prob_dict.items():\n",
    "        word_prob_dict[word] = count / total_words\n",
    "    \n",
    "    return word_prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62053f6-c816-4aa4-9470-04930cc204e2",
   "metadata": {},
   "source": [
    "**Fill Dictionary:** In this loop, I am populating the containers for the first-order Markov model and the second-order Markov model. In the first-order model, `initial`, we will have every word that begins a sentence as a key, with its value being the number of times it appears.\n",
    "\n",
    "`first_order` is a dictionary, it will have each word that appears in initial as a key, and its value will be a list of all the words that can follow it.\n",
    "\n",
    "`second_order` is a dictionary where each key is a tuple containing the two words that precede it, the first and second, or the second and third in our case, while its value, as usual, is a list composed of all the possible words that can follow this branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a46528f6-1b74-452f-b136-040b40cd81ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for line in open('robert_frost.txt', 'r'):\n",
    "    words = clean_text_and_tokenize(line)\n",
    "    num_words = len(words)\n",
    "    \n",
    "    for i in range(num_words):\n",
    "        current_word = words[i]\n",
    "        \n",
    "        if i == 0:\n",
    "            # Measure the distribution of the first word\n",
    "            initial[current_word] = initial.get(current_word, 0.) + 1\n",
    "        else:\n",
    "            previous_word = words[i - 1]\n",
    "            \n",
    "            if i == num_words - 1:\n",
    "                # Measure the probability of ending the line\n",
    "                Add2Dict(second_order, (previous_word, current_word), 'END')\n",
    "            \n",
    "            if i == 1:\n",
    "                # Measure the distribution of the second word given only the first word\n",
    "                Add2Dict(first_order, previous_word, current_word)\n",
    "            else:\n",
    "                two_words_back = words[i - 2]\n",
    "                Add2Dict(second_order, (two_words_back, previous_word), current_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38226bc-cbdf-472b-92e0-b089324d1702",
   "metadata": {},
   "source": [
    "## normalize the distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cb29f26-2d2b-43ec-b883-0f3361b761f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalize the distributions\n",
    "initial_total = sum(initial.values())\n",
    "for word, count in initial.items():\n",
    "    initial[word] = count / initial_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a85f2056-f060-4bd5-aadc-495825c96c34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for word, prob in first_order.items():\n",
    "    # Replace list with dictionary of probabilities\n",
    "    first_order[word] = List2ProbDict(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1bf5c40-e4c6-4405-9965-d0ca5b249c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key, element_list in second_order.items():\n",
    "    second_order[key] = List2ProbDict(element_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66170d9f-45b8-48c8-974b-e9994388ed0d",
   "metadata": {},
   "source": [
    "## Sampling and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b0093e6-24ad-4825-9cd9-7b9fba7f7b15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SampleWord(probability_dict):\n",
    "    random_value = np.random.random()  # valore casuale tra 0 e 1 \n",
    "    cumulative_probability = 0\n",
    "    for word, probability in probability_dict.items():\n",
    "        cumulative_probability += probability\n",
    "        if random_value < cumulative_probability:\n",
    "            return word\n",
    "    assert False, \"Should never get here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11fa7053-1d42-43f2-bf43-b4eb2517011a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LINES = 3\n",
    "\n",
    "def GenerateText():\n",
    "    for i in range(LINES):  # Generate 3 lines\n",
    "        sentence = []\n",
    "\n",
    "        # Initial word\n",
    "        first_word = SampleWord(initial)\n",
    "        sentence.append(first_word)\n",
    "\n",
    "        # Sample second word\n",
    "        second_word = SampleWord(first_order[first_word])\n",
    "        sentence.append(second_word)\n",
    "\n",
    "        # Second-order transitions until END\n",
    "        while True:\n",
    "            third_word = SampleWord(second_order[(first_word, second_word)])\n",
    "            if third_word == 'END':\n",
    "                break\n",
    "            sentence.append(third_word)\n",
    "            first_word = second_word\n",
    "            second_word = third_word\n",
    "        print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88e3c585-a3fc-4726-babb-6eb71f9b4d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but estelle dont complain shes like him there\n",
      "ill double theirs for both of them\n",
      "she hadnt found the fingerbone she wanted\n"
     ]
    }
   ],
   "source": [
    "GenerateText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1888114b-b2d5-4c88-a574-2c3ec546ed66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "come straight down off this mountain just as well off is it isnt\n",
      "one lizard at the kitchen to yourself\n",
      "it ought to have the job\n"
     ]
    }
   ],
   "source": [
    "GenerateText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdfe19e-bd98-4c56-86d7-8c2330f28a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd8a7e6-6c35-4d9a-abe9-867c0f93f223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bffd72-2380-4086-988d-9c1d44cd3c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
